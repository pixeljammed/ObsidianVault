---
tags: comp-sci, questions, uncomplete
sticker: lucide//file-question
---
## Question 1
pic

```
MERMAID HERE
```


-----
## Question 2
pic

Big data refers to the generation, storage, processing and handling of large amounts of data, such as that generated by a social media platform or a smart city. Big data is **varied** in structure, high in **volume** and generated + processed at high **velocity**. To handle such large amounts of data in a traditional way such as using a relational database would be impossible, and to store and process that data requires multiple computers (parallel processing) and thousands of terabytes of storage.

To effectively store, process and handle big data many techniques are employed such as the use of distributed database systems, functional programming (i.e: facebook using haskell for processing data) and splitting tasks across multiple instances of high-end servers / machines.

There are also various ethical issues that arise from big data handling. One concern for example would be how data is securely stored and how users are given control over what data is collected. Companies processing big data (and regular data) must adhere to various laws and guidelines such as operating under GDPR.



-----
## Question 3

pic

1. **High velocity** - if the data is being generated, processed and stored at a very quick rate (i.e: 1000s per second).
2. **High volume** - if the dataset is extremely large (i.e: 1000s of terabytes) and would not feasibly be stored on a single machine.



-----
## Question 4
pic

```
MERMAID HERE
```



-----
## Question 5
The third characteristic in this case would be **velocity** - referring to how the data would be generated and processed at an extremely fast rate.